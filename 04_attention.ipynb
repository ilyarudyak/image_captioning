{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6d1d5f-3640-4235-85ad-411b1b382ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import CaptionDataset\n",
    "from models import Encoder, Attention, DecoderWithAttention\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487cbba9-f35f-4348-973b-04a8408ec4c7",
   "metadata": {},
   "source": [
    "# 01 Math behind the attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787029f-091d-459c-9af4-2bdb2e042c4a",
   "metadata": {},
   "source": [
    "In the paper they mention that incorporating of the attention mechanism:\n",
    "> is inspired by recent success in employing attention in machine translation (Bahdanau et al., 2014)\n",
    "\n",
    "They also mention that they closely follow this paper:\n",
    "> There has been a long line of previous work incorporating attention into neural networks for vision related tasks. In particular however, our work directly extends the work of Bahdanau et al. (2014); Mnih et al. (2014); Ba et al. (2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273ef80-a80a-4117-acd1-d9e91f39f5a0",
   "metadata": {},
   "source": [
    "Let's compare the main steps of the attention in those papers. First let's have a look into *Bahdanau et al. (2014)*. In this paper we have seq-to-seq model with 2 RNNs (Encoder/Decoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b9e4b-a965-4ad4-b535-d205426ae078",
   "metadata": {},
   "source": [
    "To compute attention we need 3 steps (in notation from *Bahdanau et al. (2014)*):\n",
    "1) Compute alignment scores using an alignment model $e_{ij} = a(s_{i-1}, h_j)$. Here $h_j$ is a hidden state of *Encoder*, $s_{i-1}$ - hidden state of *Decoder*. Here $a$ is just a feedforward neural network.\n",
    "2) Compute attention weights by applying `softmax`: $\\alpha_{ij} = softmax(e)$.\n",
    "3) Compute **context vector** as a weighted sum of *Encoder's* hidden states (this is the key idea of attention - we use those hidden states of *Encoder* that are relevant for the current step of *Decoder*): $c_i = \\sum_j \\alpha_{ij} h_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3584f7d-81b2-4240-8bab-af67c30363fc",
   "metadata": {},
   "source": [
    "These are exactly the same steps that we may see in *Xu et al, 2016*:\n",
    "\n",
    "$$\n",
    "e_{ti} = f_{att}(a_i, h_{t-1}) \\\\\n",
    "\\alpha_{ti} = softmax(e) \\\\\n",
    "\\hat{z}_t = \\phi(a_i, \\alpha_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d026ff-fd1b-4b5c-9ba0-4bda5c08e813",
   "metadata": {},
   "source": [
    "In the 1st step we're using image features from CNN, not hidden states of Encoder RNN in the pevious model. $f_{att}$ is also a neural network as before. In the last step there are 2 options in the paper but in practice we're using the same sum as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a240a-cc31-47d1-a18e-9dae94009de6",
   "metadata": {},
   "source": [
    "# 02 Code for attention module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f052f-599a-485b-9601-abc44e42969b",
   "metadata": {},
   "source": [
    "Now we can easily interpret our code. The first step is to apply some linear layers (`self.encoder_att` etc.) to our input from *Encoder* (image features from CNN) and *Decoder*:\n",
    "\n",
    "```python\n",
    "# (batch_size, num_pixels, attention_dim)\n",
    "att1 = self.encoder_att(encoder_out)  \n",
    "# (batch_size, attention_dim)\n",
    "att2 = self.decoder_att(decoder_hidden)\n",
    "# (batch_size, num_pixels)  \n",
    "att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54515e4-3cd2-4364-8708-b11578550e1a",
   "metadata": {},
   "source": [
    "The next step is to apply `softmax` to get attention weights:\n",
    "\n",
    "```python\n",
    "# (batch_size, num_pixels)\n",
    "alpha = self.softmax(att)  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880b6d5-bc64-4a16-8aea-3fea706419a0",
   "metadata": {},
   "source": [
    "Finally we're using weighted sum of image features from *Encoder* to get our context vector:\n",
    "```python\n",
    "# (batch_size, encoder_dim)\n",
    "attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f660c17-3c56-447e-b43f-b9ca5d024970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba85833-f5ed-462d-be1c-6dcef660b728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56ddb8-1c28-4a8c-a091-b12c5b0a318d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d96cf5-08e9-468c-84d4-e37eefbddecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252729f-498b-453c-9911-e19157e0b2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388b275-3d94-4441-befc-bdf491e5225e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729776b-72e1-49e4-8d9a-51d21a277486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
