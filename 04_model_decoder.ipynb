{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0dca320-a875-4bc8-b912-706a40095b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import CaptionDataset\n",
    "from models import Encoder, Attention, DecoderWithAttention\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17ee00-d1cb-4f32-a1ea-640f1cd13db8",
   "metadata": {},
   "source": [
    "# 01 Dimentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ccf90-162e-438f-9586-f3813a404230",
   "metadata": {},
   "source": [
    "Let's start from understanding key dimensions that we use in `Decoder`:\n",
    "\n",
    "- `encoder_dim`: This is the last dimension of `encoder_out`, in our case 2048.\n",
    "- `decoder_dim`: This is a dimension of a hidden state of LSTM, 512.\n",
    "- `attention_dim`: Hidden size of linear layers that we use in our `Attention` module.\n",
    "- `embed_dim`: Dimension of embedded vectors that we generate for every token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8e0ab-3f61-4e7d-a1e2-c39ca25fd2b8",
   "metadata": {},
   "source": [
    "# 02 Initial state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e4d6c-9bdf-4df2-9ad8-0632848eef56",
   "metadata": {},
   "source": [
    "The logic behind building an initial state is simple (see the paper):\n",
    "> The initial memory state and hidden state of the LSTM are predicted by an average of the annotation vectors fed through two separate MLPs (`init_c` and `init_h`): \n",
    ">\n",
    ">$$c_0 = f_{init-c}(\\frac{1}{L} \\sum_i^L{a_i})$$\n",
    ">\n",
    ">$$h_0 = f_{init-h}(\\frac{1}{L} \\sum_i^L{a_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c6905-1d9d-460e-9861-5610295354e8",
   "metadata": {},
   "source": [
    "That's exactly what we're doing:\n",
    "\n",
    "```python\n",
    "# (batch_size, num_pixels, encoder_dim) -> (batch_size, encoder_dim)\n",
    "mean_encoder_out = encoder_out.mean(dim=1)\n",
    "\n",
    "# (batch_size, encoder_dim) -> (batch_size, decoder_dim)\n",
    "c = self.init_c(mean_encoder_out)\n",
    "h = self.init_h(mean_encoder_out)  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b930a-5d8a-4ea5-8907-5fa55fb85c39",
   "metadata": {},
   "source": [
    "There's no surprise here that $f_{init-c}$ and $f_{init-h}$ are just linear layers:\n",
    "\n",
    "```python\n",
    "# initial states\n",
    "# (BS, encoder_dim) -> (BS, decoder_dim)\n",
    "# linear layer to find initial hidden state of LSTMCell\n",
    "self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "# linear layer to find initial cell state of LSTMCell  \n",
    "self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65e63554-f2f3-45ba-9765-82ee278e8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderWithAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f382bf68-a1a2-4f8c-b072-ae4bf7b83cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=2048, out_features=512, bias=True),\n",
       " Linear(in_features=2048, out_features=512, bias=True))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.init_h, decoder.init_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcdd0dab-503a-4c4a-827d-fafdebea0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_out = torch.zeros(1, 196, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d12b18d-5ef4-47dd-a391-dc019956f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, c = decoder.init_hidden_state(encoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aea4233-75b5-4afe-a519-1c06972c05b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape, c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347386a-2e39-4010-87f2-071fe28ce7b8",
   "metadata": {},
   "source": [
    "# 03 `encoder_out`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0fc5a4-d2b8-4ab1-a346-5b1485639a8c",
   "metadata": {},
   "source": [
    "`encoder_out` has shape `(BS, 14, 14, 2048)` as we know. We have to change it to `(BS, 196, 2048)` - we need this for `Attention` module and for initializing `h` and `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e964c4b-7f54-4177-b104-75b491a622a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_out = torch.zeros(1, 14, 14, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a80e8664-47b2-4780-99d9-05df79b365b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_out = encoder_out.view(1, -1, 2048)  \n",
    "num_pixels = encoder_out.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47530434-1a2c-4466-a0a2-fc4cf29611bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 196, 2048]), 196)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out.shape, num_pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2798bd-1e28-423f-bf03-64c3f96cf54c",
   "metadata": {},
   "source": [
    "# 04 main loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35429a-2b8b-49d0-9332-bc4dc3c06b7d",
   "metadata": {},
   "source": [
    "## 04-1 `decode_step`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996685bf-6d69-4ba9-bc98-07d7f3570638",
   "metadata": {},
   "source": [
    "The most important thing to notice - we use a special form of input to our LSTM cell. It's not only a caption `embeddings[...]` but also a context vector. Context vector here is an output of our attention module `attention_weighted_encoding`. We use a concatenation of those 2 inputs: `torch.cat(...)`, so the dimension of our LSTM cell is `embed_dim + encoder_dim`.\n",
    "\n",
    "Probably the better names would be:\n",
    "- `embeddings` - `embedded_captions`;\n",
    "- `attention_weighted_encoding` - `context_vector`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa0155-cc5c-470b-9447-5988665879d6",
   "metadata": {},
   "source": [
    "This step looks like this in code:\n",
    "\n",
    "```python\n",
    "# decode_step - LSTM cell\n",
    "# (batch_size_t, embed_dim + encoder_dim) -> (batch_size_t, decoder_dim)\n",
    "# so we use embedded captions (only a single word at a step) and ourput of \n",
    "# our encoder after attention and translate this with LSTM cell into hidden state\n",
    "h, c = self.decode_step(\n",
    "    torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "    (h[:batch_size_t], c[:batch_size_t])\n",
    ") \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4da5b5-4f03-4067-879f-0edb3c223632",
   "metadata": {},
   "source": [
    "Here `self.decode_step` is just an LSTM cell:\n",
    "\n",
    "```python\n",
    "# this is our LSTM; we use cell in a loop\n",
    "self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818886f-aed0-46ad-aa02-898689a77c04",
   "metadata": {},
   "source": [
    "The input to a LSTM cell in the formula $(1)$ in the paper looks like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} Ey_{t-1} \\\\ h_{t-1} \\\\ \\hat{z}_t \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab374db6-4a97-408b-82f0-0b66ef79a32c",
   "metadata": {},
   "source": [
    "Let's have a look at the 1st element of this vector:\n",
    "- $y_{t-1}$ is a one-hot-encode vector from $\\mathbb{R}^K$, where $K$ is the size of the vocabulary; \n",
    "- $E \\in \\mathbb{R}^{m \\times K}$ is an embedding matrix, where $m$ is an embedding dimension; so $Ey_{t-1}$ is  an embedded vector in $\\mathbb{R}^m$; \n",
    "- in our case this is `embeddings[:batch_size_t, t, :]` with the size `embed_dim`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774caf89-cf7a-4d8e-92f3-646f090a0a81",
   "metadata": {},
   "source": [
    "Now let's have a look at the last element:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f583b8-d9c1-4e1c-a158-f124e95ebf8c",
   "metadata": {},
   "source": [
    ">$\\hat{z}_t \\in \\mathbb{R}^D$ is a\n",
    ">context vector, capturing the visual information associated with a particular input location; the extractor produces $L$ vectors, each of which is a $D$-dimensional representation corresponding to a part of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea7666-043c-44e2-bd63-727842f53547",
   "metadata": {},
   "source": [
    "- this is a result of applying an attention layer that we're considering later; \n",
    "- in our case this is `attention_weighted_encoding`; in other words this is a context vector;\n",
    "- its size is `encoder_dim` which is `2048` in our case (`encoder_out` has a dimension `(BS, 14*14, 2048)` after flattenning);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d82c4-b9a4-41b0-96f4-85f711050e5a",
   "metadata": {},
   "source": [
    "Finally we're using $h_{t-1}$ and $c_{t-1}$, in our case this is `h[:batch_size_t], c[:batch_size_t]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558df55-26cb-474e-8135-8cc37d4f557d",
   "metadata": {},
   "source": [
    "## 04-2 `predictions`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c9cfd4-7fc1-417c-8981-1daa6edb9315",
   "metadata": {},
   "source": [
    "At this stage to get predictions we use a projection from hidden space to vocabulary space. All the logic for sampling a caption (including `softmax` and `BEAM search`) is incorporated in `caption_image_beam_search()` (file `caption.py`):\n",
    "\n",
    "```python\n",
    "# this projection from hidden space onto vocabulary space\n",
    "# (batch_size_t, decoder_dim) -> (batch_size_t, vocab_size)\n",
    "# we don't compute softmax here or choose max value\n",
    "preds = self.fc(self.dropout(h)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b99aa-69ca-4436-8fc0-517504d8ea15",
   "metadata": {},
   "source": [
    "Here `self.fc` is just a Linear layer `decoder_dim -> vocab_size`:\n",
    "\n",
    "```python\n",
    "self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
